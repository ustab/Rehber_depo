{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ustab/Rehber_depo/blob/main/PreprocessingPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metin ön işleme adımları ve evrensel ardışık düzen\n",
        "Herhangi bir ML modeline bir tür veri beslemeden önce, düzgün bir şekilde ön işleme tabi tutulmalıdır. Sözcüğü duymuş olmalısınız: Garbage in, garbage out(GIGO). Metin belirli bir veri türüdür ve çoğu ML modeline doğrudan beslenemez, bu nedenle onu bir modele beslemeden önce bir şekilde ondan sayısal özellikler çıkarmanız gerekir, başka bir deyişle vectorize. Vektörleştirme bu öğreticinin konusu değildir, ancak anlamanız gereken en önemli şey, GIGO'nun vektörleştirmeye de uygulanabilir olduğudur, nitel özellikleri yalnızca niteliksel olarak önceden işlenmiş metinden çıkarabilirsiniz.\n",
        "\n",
        "Tartışacağımız şeyler:\n",
        "\n",
        "Tokenizasyon\n",
        "Temizlik\n",
        "normalleştirme\n",
        "Lemmatizasyon\n",
        "buharda pişirme\n",
        "Son olarak, uygulamalarınızda kullanabileceğiniz yeniden kullanılabilir ardışık düzen oluşturacağız."
      ],
      "metadata": {
        "id": "6d9c8iixOyh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "fqPzz3d4OCwd",
        "outputId": "613c2afc-0524-49e5-f6a1-2cdbff1e2d46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'##example_text = An explosion targeting a tourist bus has injured at least 16 people near the Grand Egyptian Museum, \\nnext to the pyramids in Giza, security sources say E.U.\\n\\nSouth African tourists are among the injured. Most of those hurt suffered minor injuries, \\nwhile three were treated in hospital, N.A.T.O. say.\\n\\nhttp://localhost:8888/notebooks/Text%20preprocessing.ipynb\\n\\n@nickname of twitter user and his email is email@gmail.com . \\n\\nA device went off close to the museum fence as the bus was passing on 16/02/2012'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "'''##example_text = An explosion targeting a tourist bus has injured at least 16 people near the Grand Egyptian Museum, \n",
        "next to the pyramids in Giza, security sources say E.U.\n",
        "\n",
        "South African tourists are among the injured. Most of those hurt suffered minor injuries, \n",
        "while three were treated in hospital, N.A.T.O. say.\n",
        "\n",
        "http://localhost:8888/notebooks/Text%20preprocessing.ipynb\n",
        "\n",
        "@nickname of twitter user and his email is email@gmail.com . \n",
        "\n",
        "A device went off close to the museum fence as the bus was passing on 16/02/2012'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wnIeA5oOCwe"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "`Tokenization` - text preprocessing step, which assumes splitting text into `tokens`(words, senteces, etc.)\n",
        "\n",
        "Seems like you can use somkeind of simple seperator to achieve it, but you don't have to forget that there are a lot of different situations, where separators just don't work. For example, `.` separator for tokenization into sentences will fail if you have abbreviations with dots. So you have to have more complex model to achieve good enough result. Commonly this problem is solved using `nltk` or `spacy` nlp libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rak8H1ZbQJ-f",
        "outputId": "e79e4403-cc4f-401d-d2ad-4be580b09967"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "CO6Afft3OCwf",
        "outputId": "3d5dedd0-0c75-42e5-cdb9-d96eb4f8de0c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Tokenized words: ['An', 'explosion', 'targeting', 'a', 'tourist', 'bus', 'has', 'injured', 'at', 'least', '16', 'people', 'near', 'the', 'Grand', 'Egyptian', 'Museum', ',', 'next', 'to', 'the', 'pyramids', 'in', 'Giza', ',', 'security', 'sources', 'say', 'E.U', '.', 'South', 'African', 'tourists', 'are', 'among', 'the', 'injured', '.', 'Most', 'of', 'those', 'hurt', 'suffered', 'minor', 'injuries', ',', 'while', 'three', 'were', 'treated', 'in', 'hospital', ',', 'N.A.T.O', '.', 'say', '.', 'http', ':', '//localhost:8888/notebooks/Text', '%', '20preprocessing.ipynb', '@', 'nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email', '@', 'gmail.com', '.', 'A', 'device', 'went', 'off', 'close', 'to', 'the', 'museum', 'fence', 'as', 'the', 'bus', 'was', 'passing', 'on', '16/02/2012', '.']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk_words = word_tokenize(example_text)\n",
        "display(f\"Tokenized words: {nltk_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "9uyIVQufOCwg",
        "outputId": "d43426f4-1d1a-4b9e-d57e-dca7f68e30e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"In spacy but not in nltk: {'/', 'E.U.', 'N.A.T.O.', 'notebooks', 'http://localhost:8888', '\\\\n\\\\n', '\\\\n', 'email@gmail.com', '@nickname', 'Text%20preprocessing.ipynb'}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"In nltk but not in spacy: {'20preprocessing.ipynb', '//localhost:8888/notebooks/Text', 'http', 'N.A.T.O', 'gmail.com', ':', '%', 'E.U', 'nickname', '@'}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(f\"In spacy but not in nltk: {set(spacy_words).difference(set(nltk_words))}\")\n",
        "display(f\"In nltk but not in spacy: {set(nltk_words).difference(set(spacy_words))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0TQrOGOCwh"
      },
      "source": [
        "We see that `spacy` tokenized some weird staff like `\\n`, `\\n\\n`, but was able to handle urls, emails and twitter-like mentions. Also we see that `nltk` tokenized abbreviations without the last `.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rseTxz9VOCwi"
      },
      "source": [
        "# Cleaning\n",
        "\n",
        "`Cleaning` step assumes removing all undesirable content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5H6yI9_OCwi"
      },
      "source": [
        "### Punctuation removal\n",
        "`Punctuation removal` might be a good step, when punctuation does not brings additional value for text vectorization. Punctuation removal is better to be done after tokenization step, doing it before might cause undesirable effects. Good choice for `TF-IDF`, `Count`, `Binary` vectorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nPRPvfqcOCwj",
        "outputId": "a3166489-f46d-47c1-d198-a5ab254ff7eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Punctuation symbols: !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "display(f\"Punctuation symbols: {string.punctuation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kf6NA0CVOCwj"
      },
      "outputs": [],
      "source": [
        "text_with_punct = \"@nickname of twitter user, and his email is email@gmail.com .\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Lf_HPXIWOCwj",
        "outputId": "065d01a7-f9f8-4f02-c02d-c55cd7b1a327"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Text without punctuation: nickname of twitter user and his email is emailgmailcom '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "text_without_punct = text_with_punct.translate(str.maketrans('', '', string.punctuation))\n",
        "display(f\"Text without punctuation: {text_without_punct}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJQTTHKzOCwk"
      },
      "source": [
        "Here you can see that important symbols for correct tokenizations were removed. Now email can't be properly detected. As you could mention from the `Tokenization` step, punctuation symbors were parsed as single tokens, so better way would be to tokenize first and then remove punctuation symbols. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "P8o4EjqEOCwk",
        "outputId": "f5b191d3-183f-41d7-987b-3aeb5c54298c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Python based removal: ['@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Spacy based removal: ['@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "doc = nlp(text_with_punct)\n",
        "tokens = [t.text for t in doc]\n",
        "# python \n",
        "tokens_without_punct_python = [t for t in tokens if t not in string.punctuation]\n",
        "display(f\"Python based removal: {tokens_without_punct_python}\")\n",
        "\n",
        "tokens_without_punct_spacy = [t.text for t in doc if t.pos_ != 'PUNCT']\n",
        "display(f\"Spacy based removal: {tokens_without_punct_spacy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsskBIBROCwk"
      },
      "source": [
        "Here you see that `python-based` removal worked even better than spacy, because spacy tagged `@nicname` as `PUNCT` part-of-speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPuLupZrOCwk"
      },
      "source": [
        "### Stop words removal\n",
        "\n",
        "`Stop words` usually refers to the most common words in a language, which usualy does not bring additional meaning. There is no single universal list of stop words used by all nlp tools, because this term has very fuzzy definition. Although practice has shown, that this step is much have, when preparing text for indexing, but might be tricky for text classification purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-vXc7dEMOCwl"
      },
      "outputs": [],
      "source": [
        "text = \"This movie is just not good enough\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dkIO0Qo9OCwl",
        "outputId": "19051ebf-ad26-4960-cb3e-9f879d4766e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Spacy stop words count: 326'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "display(f\"Spacy stop words count: {len(spacy_stop_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Mup5c1RcOCwl",
        "outputId": "d78e07c5-8785-416d-e5fa-4f6251fd4de0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Spacy text without stop words: ['movie', 'good']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\n",
        "display(f\"Spacy text without stop words: {text_without_stop_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDe_EXfNOCwm"
      },
      "source": [
        "Here you see that nltk and spacy has different vocabulary size, so the results of filtering are different. But the main thing I want to underline is that the word `not` was filtered, which in the most cases will be allright, but in the case when you want determine the polarity of this sentence `not` will bring the additional meaning.\n",
        "\n",
        "For such cases you are able to set stop words you can ignore in spacy library. In the case of nltk you cat just remove or add custom words to `nltk_stop_words`, it is just a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N-gMTFZyOCwm",
        "outputId": "27cb1cd5-7792-4a12-c24a-42405b5e4366"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Spacy text without updated stop words: ['movie', 'not', 'good']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "customize_stop_words = [\n",
        "    'not'\n",
        "]\n",
        "\n",
        "for w in customize_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\n",
        "display(f\"Spacy text without updated stop words: {text_without_stop_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44JM4HAyOCwm"
      },
      "source": [
        "# Normalization\n",
        "\n",
        "Like any data text requires normalization. In case of text it is:\n",
        "\n",
        "1. Converting dates to text\n",
        "2. Numbers to text\n",
        "3. Currency/Percent signs to text\n",
        "4. Expanding of abbreviations (content dependent) NLP - Natural Language Processing, Neuro-linguistic programming, Non-Linear programming\n",
        "5. Spelling mistakes correction\n",
        "\n",
        "To summarize, normalization is a convertion of any non-text information into textual equivalent.\n",
        "\n",
        "For this purposes exists a great library - [normalize](https://github.com/EFord36/normalise). I'll show you usage of this library from its README. This library is based on `nltk` package, so it expects `nltk` word tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "0RlN0IABOCwm",
        "outputId": "db269fb1-4a0f-4702-c154-9de7e4301fb7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1b950c30f930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m text = \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0mOn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;36m13\u001b[0m \u001b[0mFeb\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;36m2007\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheresa\u001b[0m \u001b[0mMay\u001b[0m \u001b[0mannounced\u001b[0m \u001b[0mon\u001b[0m \u001b[0mMTV\u001b[0m \u001b[0mnews\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mof\u001b[0m \u001b[0mchildhod\u001b[0m \u001b[0mobesity\u001b[0m \u001b[0mhad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrisen\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;36m7.3\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9.6\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjust\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0myears\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcosting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m£\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'normalise'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from normalise import normalise\n",
        "\n",
        "text = \"\"\"\n",
        "On the 13 Feb. 2007, Theresa May announced on MTV news that the rate of childhod obesity had \n",
        "risen from 7.3-9.6% in just 3 years , costing the N.A.T.O £20m\n",
        "\"\"\"\n",
        "\n",
        "user_abbr = {\n",
        "    \"N.A.T.O\": \"North Atlantic Treaty Organization\"\n",
        "}\n",
        "\n",
        "normalized_tokens = normalise(word_tokenize(text), user_abbrevs=user_abbr, verbose=False)\n",
        "display(f\"Normalized text: {' '.join(normalized_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVHn8zqOCwm"
      },
      "source": [
        "The worst thing in this library is that for now you can't disable some modules, like abbreviation expanding, and int causes things like `MTV` -> `M T V`. But I have already added an appropriate issue on this repository, maybe it would be fixed in a while."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMvcFNsmOCwn"
      },
      "source": [
        "# Lematization and Steaming\n",
        "\n",
        "`Stemming` is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. \n",
        "\n",
        "`Lemmatization`, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "dMwk-nwGOCwn",
        "outputId": "58878c2d-2d9a-43c1-ee8d-924ea7774a78"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d7fa828e55f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'normalized_tokens' is not defined"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "\n",
        "text = ' '.join(normalized_tokens)\n",
        "tokens = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VtVnHi71OCwn",
        "outputId": "263179fc-aefe-4146-cab7-f6ab514b96c9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Stemed text: @nicknam of twitter user , and hi email is email@gmail.com .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "porter=PorterStemmer()\n",
        "stem_words = np.vectorize(porter.stem)\n",
        "stemed_text = ' '.join(stem_words(tokens))\n",
        "display(f\"Stemed text: {stemed_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSM2xspZOCwn",
        "outputId": "e8397b75-4a90-4935-fd05-1365da34246b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'nltk lemmatized text: On the thirteenth of February two thousand and seven , Theresa May announced on M T V news that the rate of childhood obesity had risen from seven point three to nine point six % in just three year , costing the North Atlantic Treaty Organization twenty million pound'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\n",
        "lemmatized_text = ' '.join(lemmatize_words(tokens))\n",
        "display(f\"nltk lemmatized text: {lemmatized_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhuHoF2pOCwn",
        "outputId": "d9227c3c-bde9-4052-d8fd-752ab774733c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Spacy lemmatized text: On the thirteenth of February two thousand and seven , Theresa May announce on M T v news that the rate of childhood obesity have rise from seven point three to nine point six % in just three year , cost the North Atlantic Treaty Organization twenty million pound'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "lemmas = [t.lemma_ for t in nlp(text)]\n",
        "display(f\"Spacy lemmatized text: {' '.join(lemmas)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4PXXXCJOCwn"
      },
      "source": [
        "We see that `spacy` lemmatized much better than nltk, one of examples `risen` -> `rise`, only `spacy` handeled that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUjIu5c0OCwo"
      },
      "source": [
        "# Reusable pipeline\n",
        "\n",
        "And now my favourite part! We are going to cretate reusable pipeline, which you could use on any of you projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "HELEc8v8OCwo",
        "outputId": "9c0b9afe-74a5-4e6c-8ffc-2d2dda4edcf1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-43b46ed72437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnormalise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'normalise'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "import string\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from normalise import normalise\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 variety=\"BrE\",\n",
        "                 user_abbrevs={},\n",
        "                 n_jobs=1):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \n",
        "        variety - format of date (AmE - american type, BrE - british format) \n",
        "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
        "        n_jobs - parallel jobs to run\n",
        "        \"\"\"\n",
        "        self.variety = variety\n",
        "        self.user_abbrevs = user_abbrevs\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        partitions = 1\n",
        "        cores = mp.cpu_count()\n",
        "        if self.n_jobs <= -1:\n",
        "            partitions = cores\n",
        "        elif self.n_jobs <= 0:\n",
        "            return X_copy.apply(self._preprocess_text)\n",
        "        else:\n",
        "            partitions = min(self.n_jobs, cores)\n",
        "\n",
        "        data_split = np.array_split(X_copy, partitions)\n",
        "        pool = mp.Pool(cores)\n",
        "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        # some issues in normalise package\n",
        "        try:\n",
        "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h_iVSex8OCwo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_bbc = pd.read_csv('/content/bbc-text.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "aDW3O-rkOCwo",
        "outputId": "79a08dbc-40df-41e6-871a-acf06459a0f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TextPreprocessor' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "text = TextPreprocessor(n_jobs=-1).transform(df_bbc['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asg2UicMOCwp",
        "outputId": "27acd898-4daf-4020-dfd8-3f5ea6ec8123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance of transformer on 2225 texts and 2 processes\n"
          ]
        }
      ],
      "source": [
        "print(f\"Performance of transformer on {len(df_bbc)} texts and {mp.cpu_count()} processes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAB8pi5qOCwp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}